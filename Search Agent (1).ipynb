{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85802f31-afdd-4ce1-bab2-6d745a3793ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Agent notebook\n",
    "\n",
    "This is an auto-generated notebook created by an AI Playground export. We generated three notebooks in the same folder:\n",
    "- [**agent**]($./agent): contains the code to build the agent.\n",
    "- [config.yml]($./config.yml): contains the configurations.\n",
    "- [driver]($./driver): logs, evaluate, registers, and deploys the agent.\n",
    "\n",
    "This notebook uses Mosaic AI Agent Framework ([AWS](https://docs.databricks.com/en/generative-ai/retrieval-augmented-generation.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/retrieval-augmented-generation)) to recreate your agent from the AI Playground. It defines a LangChain agent that has access to the tools from the source Playground session.\n",
    "\n",
    "Use this notebook to iterate on and modify the agent. For example, you could add more tools or change the system prompt.\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Address all `TODO`s in this notebook.\n",
    "- Review the contents of [config.yml]($./config.yml) as it defines the tools available to your agent, the LLM endpoint, and the agent prompt.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "After testing and iterating on your agent in this notebook, go to the auto-generated [driver]($./driver) notebook in this folder to log, register, and deploy the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fddb84a-1d03-40db-8abe-73deb6b83de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqqq mlflow-skinny langchain==0.2.16 langgraph-checkpoint==1.0.12 langchain_core langchain-community==0.2.16 langgraph==0.2.16 pydantic\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a438b2f-2d13-4fdf-b7f2-13bd821e4c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import and setup\n",
    "\n",
    "Use `mlflow.langchain.autolog()` to set up [MLflow traces](https://docs.databricks.com/en/mlflow/mlflow-tracing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04b32e2-235c-42f6-881f-8e96ae3a76ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "config = ModelConfig(development_config=\"config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a844379c-7a01-49d4-be0e-53ea0daa1ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the chat model and tools\n",
    "Create a LangChain chat model that supports [LangGraph tool](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/) calling.\n",
    "\n",
    "Modify the tools your agent has access to by modifying the `uc_functions` list in [config.yml]($./config.yml). Any non-UC function spec tools can be defined in this notebook. See [LangChain - How to create tools](https://python.langchain.com/v0.2/docs/how_to/custom_tools/) and [LangChain - Using built-in tools](https://python.langchain.com/v0.2/docs/how_to/tools_builtin/).\n",
    "\n",
    " **_NOTE:_**  This notebook uses LangChain, however AI Agent Framework is compatible with other agent frameworks like Pyfunc and LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e415c7-72dd-433b-9198-130953b3a35c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.tools.databricks import UCFunctionToolkit\n",
    "\n",
    "# Create the llm\n",
    "llm = ChatDatabricks(endpoint=config.get(\"llm_endpoint\"))\n",
    "\n",
    "uc_functions = config.get(\"uc_functions\")\n",
    "\n",
    "uc_function_tools = (\n",
    "    UCFunctionToolkit(warehouse_id=config.get(\"warehouse_id\"))\n",
    "    .include(*uc_functions)\n",
    "    .get_tools()\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: fill in implementations for functions search_databricks_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fb382b1-4302-43b3-aed7-d9f8e40abfb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Output parsers\n",
    "Databricks interfaces, such as the AI Playground, can optionally display pretty-printed tool calls.\n",
    "\n",
    "Use the following helper functions to parse the LLM's output into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02885b23-8c63-4f7a-8306-c7b7df6f5948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict, Any\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    "    MessageLikeRepresentation,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def stringify_tool_call(tool_call: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert a raw tool call into a formatted string that the playground UI expects if there is enough information in the tool_call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = json.dumps(\n",
    "            {\n",
    "                \"id\": tool_call.get(\"id\"),\n",
    "                \"name\": tool_call.get(\"name\"),\n",
    "                \"arguments\": json.dumps(tool_call.get(\"args\", {})),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "        return f\"<tool_call>{request}</tool_call>\"\n",
    "    except:\n",
    "        return str(tool_call)\n",
    "\n",
    "\n",
    "def stringify_tool_result(tool_msg: ToolMessage) -> str:\n",
    "    \"\"\"\n",
    "    Convert a ToolMessage into a formatted string that the playground UI expects if there is enough information in the ToolMessage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = json.dumps(\n",
    "            {\"id\": tool_msg.tool_call_id, \"content\": tool_msg.content}, indent=2\n",
    "        )\n",
    "        return f\"<tool_call_result>{result}</tool_call_result>\"\n",
    "    except:\n",
    "        return str(tool_msg)\n",
    "\n",
    "\n",
    "def parse_message(msg) -> str:\n",
    "    \"\"\"Parse different message types into their string representations\"\"\"\n",
    "    # tool call result\n",
    "    if isinstance(msg, ToolMessage):\n",
    "        return stringify_tool_result(msg)\n",
    "    # tool call\n",
    "    elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "        tool_call_results = [stringify_tool_call(call) for call in msg.tool_calls]\n",
    "        return \"\".join(tool_call_results)\n",
    "    # normal HumanMessage or AIMessage (reasoning or final answer)\n",
    "    elif isinstance(msg, (AIMessage, HumanMessage)):\n",
    "        return msg.content\n",
    "    else:\n",
    "        print(f\"Unexpected message type: {type(msg)}\")\n",
    "        return str(msg)\n",
    "\n",
    "\n",
    "def wrap_output(stream: Iterator[MessageLikeRepresentation]) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Process and yield formatted outputs from the message stream.\n",
    "    The invoke and stream langchain functions produce different output formats.\n",
    "    This function handles both cases.\n",
    "    \"\"\"\n",
    "    for event in stream:\n",
    "        # the agent was called with invoke()\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                yield parse_message(msg) + \"\\n\\n\"\n",
    "        # the agent was called with stream()\n",
    "        else:\n",
    "            for node in event:\n",
    "                for key, messages in event[node].items():\n",
    "                    if isinstance(messages, list):\n",
    "                        for msg in messages:\n",
    "                            yield parse_message(msg) + \"\\n\\n\"\n",
    "                    else:\n",
    "                        print(\"Unexpected value {messages} for key {key}. Expected a list of `MessageLikeRepresentation`'s\")\n",
    "                        yield str(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "832eb7ce-d3bf-4160-8176-2c41af3dc87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the agent\n",
    "Here we provide a simple graph that uses the model and tools defined by [config.yml]($./config.yml). This graph is adapated from [this LangGraph guide](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/).\n",
    "\n",
    "\n",
    "To further customize your LangGraph agent, you can refer to:\n",
    "* [LangGraph - Quick Start](https://langchain-ai.github.io/langgraph/tutorials/introduction/) for explanations of the concepts used in this LangGraph agent\n",
    "* [LangGraph - How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/) to expand the functionality of your agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0889077a-3606-48d8-a2c9-ecaf2685b6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.functions import col  \n",
    "\n",
    "class DataFetchingAgent:  \n",
    "    def __init__(self, catalog: str, schema: str, index_name: str):  \n",
    "        self.catalog = catalog  \n",
    "        self.schema = schema  \n",
    "        self.index_name = index_name  \n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"DataFetchingAgent\") \\\n",
    "            .getOrCreate()  \n",
    "    \n",
    "    def fetch_data(self, query: str):  \n",
    "        try:  \n",
    "            # Assuming the index is registered in the metastore  \n",
    "            index_path = \"databrickshackathon.default.spentinsights\"             \n",
    "            df = self.spark.table(index_path).filter(col(\"content\").contains(query))  \n",
    "            return df.collect()  \n",
    "        except Exception as e:  \n",
    "            return f\"Error fetching data: {str(e)}\"  \n",
    "\n",
    "# We create the AgentState that we will pass around\n",
    "# This simply involves a list of messages\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolExecutor, Sequence[Sequence[DataFetchingAgent]\n",
    " ]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there is no function call, then we finish\n",
    "        if not last_message.tool_calls:\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = SystemMessage(content=agent_prompt)\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    # Define the function that calls the model\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        # First, we define the start node. We use agent.\n",
    "        # This means these are the edges taken after the agent node is called.\n",
    "        \"agent\",\n",
    "        # Next, we pass in the function that will determine which node is called next.\n",
    "        should_continue,\n",
    "        # The mapping below will be used to determine which node to go to\n",
    "        {\n",
    "            # If tools, then we call the tool node.\n",
    "            \"continue\": \"tools\",\n",
    "            # END is a special node marking that the graph should finish.\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    # We now add a unconditional edge from tools to agent.\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d82dd7-f4cb-4c60-bc3c-6f4665f584ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from mlflow.langchain.output_parsers import ChatCompletionsOutputParser\n",
    "\n",
    "# Create the agent with the system message if it exists\n",
    "try:\n",
    "    agent_prompt = config.get(\"agent_prompt\")\n",
    "    agent_with_raw_output = create_tool_calling_agent(\n",
    "        llm, uc_function_tools, agent_prompt=agent_prompt\n",
    "    )\n",
    "except KeyError:\n",
    "    agent_with_raw_output = create_tool_calling_agent(llm, uc_function_tools)\n",
    "agent = agent_with_raw_output | RunnableGenerator(wrap_output) | ChatCompletionsOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce15163-c0cf-484b-9fc3-0ec26a948583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since this notebook called `mlflow.langchain.autolog()` you can view the trace for each step the agent takes.\n",
    "\n",
    "Replace this placeholder input with an appropriate domain-specific example for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4bde92-8ad5-4f61-ad94-f5bd55f7398a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"The cheapest laptop can vary depending on several factors such as the retailer, location, and availability. However, I can give you some general ideas on what to expect.\\n\\n**Under $200:**\\n\\n* Acer Aspire 3: A basic laptop with an Intel Celeron processor, 4GB of RAM, and a 64GB eMMC storage. ($150-$200)\\n* Lenovo IdeaPad 130S: A compact laptop with an Intel Celeron processor, 4GB of RAM, and a 64GB eMMC storage. ($150-$200)\\n* HP Stream 14: A budget-friendly laptop with an Intel Celeron processor, 4GB of RAM, and a 64GB eMMC storage. ($150-$200)\\n\\n**Under $300:**\\n\\n* Acer Aspire E 15: A budget laptop with an Intel Core i3 processor, 6GB of RAM, and a 256GB hard drive. ($250-$300)\\n* Lenovo IdeaPad 330S: A slim laptop with an Intel Core i3 processor, 4GB of RAM, and a 256GB hard drive. ($250-$300)\\n* Dell Inspiron 15 3000: A budget laptop with an Intel Core i3 processor, 4GB of RAM, and a 256GB hard drive. ($250-$300)\\n\\n**Under $400:**\\n\\n* Asus Vivobook X512FA: A budget laptop with an Intel Core i3 processor, 8GB of RAM, and a 256GB hard drive. ($350-$400)\\n* HP Envy x360: A convertible laptop with an AMD Ryzen 3 processor, 8GB of RAM, and a 256GB hard drive. ($350-$400)\\n* Lenovo Yoga C340: A convertible laptop with an Intel Core i3 processor, 8GB of RAM, and a 256GB hard drive. ($350-$400)\\n\\nPlease note that these prices are estimates and may vary depending on the retailer, location, and availability. Additionally, these laptops may not have all the features you need, so be sure to check the specifications before making a purchase.\\n\\nIt's also worth considering refurbished or used laptops, which can be significantly cheaper than brand new ones. However, be sure to purchase from a reputable seller and check the warranty and return policies before making a purchase.\\n\\n\"}, 'finish_reason': 'stop'}], 'object': 'chat.completion'} ------------------------------------------------------------\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-f8537f0adb6e4fbb83cd891936113583\"",
      "text/plain": [
       "Trace(request_id=tr-f8537f0adb6e4fbb83cd891936113583)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: replace this placeholder input example with an appropriate domain-specific example for your agent\n",
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"What is the cheapest laptop?\"}]}):\n",
    "    print(event, \"---\" * 20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ad18c0-17da-4c8f-b253-4f0a39898af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e5b125-b3bc-4ec3-a18d-803ba62da829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "You can rerun the cells above to iterate and test the agent.\n",
    "\n",
    "Go to the auto-generated [driver]($./driver) notebook in this folder to log, register, and deploy the agent."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Search Agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
